
HF_CACHE_PATH=~/.cache/huggingface
HF_TOKEN=   # required if you want to access a gated model

# inference server
INFERENCE_SERVER_PORT=50000

MODEL=meta-llama/Llama-3.1-70B-Instruct
MAX_MODEL_LEN=4096 # context length
GPU_COUNT=1 # number of GPUs to use
TENSOR_PARALLEL_SIZE=1 # should be equal to GPU_COUNT

# Sui Configuration
SUI_CONFIG_PATH=~/.sui/sui_config

# Atoma Node Service Configuration
ATOMA_SERVICE_PORT=3000
