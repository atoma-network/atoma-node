
HF_CACHE_PATH=~/.cache/huggingface
HF_TOKEN=   # required if you want to access a gated model

# ----------------------------------------------------------------------------------
# atoma node configuration

# Sui Configuration
SUI_CONFIG_PATH=~/.sui/sui_config

# Atoma Node Service Configuration
ATOMA_SERVICE_PORT=3000

TRACE_LEVEL=info

# ----------------------------------------------------------------------------------
# chat completions server
CHAT_COMPLETIONS_ON=true
CHAT_COMPLETIONS_BACKEND=vllm
CHAT_COMPLETIONS_SERVER_PORT=50000
CHAT_COMPLETIONS_MODEL=meta-llama/Llama-3.1-70B-Instruct
CHAT_COMPLETIONS_MAX_MODEL_LEN=4096 # context length

# vllm backend
VLLM_TENSOR_PARALLEL_SIZE=1 # should be equal to GPU_COUNT

# ----------------------------------------------------------------------------------
# embeddings server
EMBEDDINGS_ON=true
EMBEDDINGS_BACKEND=tei
EMBEDDINGS_SERVER_PORT=50001
EMBEDDINGS_MODEL=intfloat/multilingual-e5-large-instruct

# tei backend
# Choose one of these based on your GPU architecture:
# CPU:                                  TEI_IMAGE=ghcr.io/huggingface/text-embeddings-inference:cpu-1.5
# Volta:                                UNSUPPORTED
# Turing (T4, RTX 2000 series, ...):    TEI_IMAGE=ghcr.io/huggingface/text-embeddings-inference:turing-1.5
# Ampere 80 (A100, A30):                TEI_IMAGE=ghcr.io/huggingface/text-embeddings-inference:1.5
# Ampere 86 (A10, A40, ...):            TEI_IMAGE=ghcr.io/huggingface/text-embeddings-inference:86-1.5
# Ada Lovelace (RTX 4000 series, ...):  TEI_IMAGE=ghcr.io/huggingface/text-embeddings-inference:89-1.5
# Hopper (H100):                        TEI_IMAGE=ghcr.io/huggingface/text-embeddings-inference:hopper-1.5
TEI_IMAGE=ghcr.io/huggingface/text-embeddings-inference:1.5

# ----------------------------------------------------------------------------------

