services:
  vllm:
    profiles: [chat_completions_vllm]
    image: vllm/vllm-openai:v0.6.3
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    volumes:
      - ${HF_CACHE_PATH:-~/.cache/huggingface}:/root/.cache/huggingface
    env_file:
      - .env
    ports:
      - "${CHAT_COMPLETIONS_SERVER_PORT}:8000"
    ipc: host
    command: --model ${CHAT_COMPLETIONS_MODEL} --max-model-len ${CHAT_COMPLETIONS_MAX_MODEL_LEN} --tensor-parallel-size ${VLLM_TENSOR_PARALLEL_SIZE}

  tei:
    profiles: [embeddings_tei]
    image: ${TEI_IMAGE}
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    volumes:
      - ${HF_CACHE_PATH:-~/.cache/huggingface}:/root/.cache/huggingface
    env_file:
      - .env
    ports:
      - "${EMBEDDINGS_SERVER_PORT}:80"
    command: --model-id ${EMBEDDINGS_MODEL} --huggingface-hub-cache /root/.cache/huggingface/hub

  mistralrs:
    profiles: [image_generations_mistral]
    image: ${MISTRALRS_IMAGE}
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    volumes:
      - ${HF_CACHE_PATH:-~/.cache/huggingface}:/root/.cache/huggingface
    env_file:
      - .env
    ports:
      - "${IMAGE_GENERATIONS_SERVER_PORT}:80"
    command: diffusion-plain -m ${IMAGE_GENERATIONS_MODEL} -a flux-offloaded
