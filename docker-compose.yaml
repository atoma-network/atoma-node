# Base configuration for Atoma services
x-atoma-node: &atoma-node
  image: ghcr.io/atoma-network/atoma-node:latest
  build:
    context: .
    dockerfile: Dockerfile
  platform: ${PLATFORM:-} # Will be empty if not set
  runtime: nvidia
  deploy:
    resources:
      reservations:
        devices:
          - driver: nvidia
            count: all
            capabilities: [gpu]
  volumes:
    - ${CONFIG_PATH:-./config.toml}:/app/config.toml
    - ./logs:/app/logs
    - sui-config-volume:/root/.sui/sui_config
    - ${SUI_CONFIG_PATH:-~/.sui/sui_config}:/tmp/.sui/sui_config
    - ./data:/app/data
  env_file: .env
  environment:
    - RUST_LOG=${ATOMA_LOG_LEVELS:-info}
  networks:
    - atoma-network

# Base configuration for cuda inference services
x-inference-service-cuda: &inference-service-cuda
  runtime: nvidia
  deploy:
    resources:
      reservations:
        devices:
          - driver: nvidia
            count: all
            capabilities: [gpu]
  volumes:
    - ${HF_CACHE_PATH:-~/.cache/huggingface}:/root/.cache/huggingface
  env_file: .env
  networks:
    - atoma-network
  logging:
    driver: "json-file"
    options:
      max-size: "100m"
      max-file: "3"
  environment:
    - OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4317
    - OTEL_SERVICE_NAME=vllm
    - OTEL_LOGS_EXPORTER=otlp

# Base configuration for Atoma services
x-atoma-node-no-nvidia: &atoma-node-no-nvidia
  image: ghcr.io/atoma-network/atoma-node:latest
  build:
    context: .
    dockerfile: Dockerfile
  platform: ${PLATFORM:-} # Will be empty if not set
  volumes:
    - ${CONFIG_PATH:-./config.toml}:/app/config.toml
    - ./logs:/app/logs
    - sui-config-volume:/root/.sui/sui_config
    - ${SUI_CONFIG_PATH:-~/.sui/sui_config}:/tmp/.sui/sui_config
    - ./data:/app/data
  env_file: .env
  environment:
    - RUST_LOG=${ATOMA_LOG_LEVELS:-info}
  networks:
    - atoma-network

# Base configuration for cpu inference services
x-inference-service-cpu: &inference-service-cpu
  volumes:
    - ${HF_CACHE_PATH:-~/.cache/huggingface}:/root/.cache/huggingface
  env_file: .env
  networks:
    - atoma-network

# Base configuration for ROCm inference services
x-inference-service-rocm: &inference-service-rocm
  runtime: rocm
  devices:
    - all
  volumes:
    - ${HF_CACHE_PATH:-~/.cache/huggingface}:/root/.cache/huggingface
  env_file: .env
  networks:
    - atoma-network

services:
  postgres-db:
    image: postgres:13
    platform: ${PLATFORM:-} # Will be empty if not set
    restart: always
    environment:
      - POSTGRES_DB=${POSTGRES_DB}
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
    volumes:
      - postgres-data:/var/lib/postgresql/data
    ports:
      - "${POSTGRES_PORT:-5432}:5432"
    env_file: .env
    networks:
      - atoma-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB} -p 5432 || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5

  prometheus:
    image: prom/prometheus:v3.1.0
    platform: ${PLATFORM:-}
    restart: always
    ports:
      - "${PROMETHEUS_PORT:-9090}:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus-data:/prometheus
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
      - "--web.enable-lifecycle"
    env_file: .env
    networks:
      - atoma-network
    extra_hosts:
      - "host.docker.internal:host-gateway"
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 10s
      timeout: 5s
      retries: 5

  grafana:
    image: grafana/grafana:11.5.1
    platform: ${PLATFORM:-}
    restart: always
    ports:
      - "${GRAFANA_PORT:-30001}:3000"
    depends_on:
      - prometheus
    volumes:
      - grafana_data:/var/lib/grafana
    env_file: .env
    environment:
      - GF_SERVER_ROOT_URL=http://${GRAFANA_DOMAIN}:${GRAFANA_PORT:-30001}/
    networks:
      - atoma-network
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:3000/api/health || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5

  loki:
    image: grafana/loki:2.9.4
    platform: ${PLATFORM:-}
    ports:
      - "3100:3100"
    volumes:
      - ./loki.yaml:/etc/loki/loki.yaml
      - loki-data:/loki
    command: -config.file=/etc/loki/loki.yaml
    networks:
      - atoma-network
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:3100/ready || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5

  tempo:
    image: grafana/tempo:2.7.0
    platform: ${PLATFORM:-}
    command: ["-config.file=/etc/tempo.yaml"]
    volumes:
      - ./tempo.yaml:/etc/tempo.yaml
      - tempo-data:/tmp/tempo
    ports:
      - "3200:3200"
    networks:
      - atoma-network
    user: "0"
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:3200/status"]
      interval: 10s
      timeout: 5s
      retries: 5

  node-exporter:
    image: prom/node-exporter:v1.7.0
    container_name: node-exporter
    restart: unless-stopped
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - "--path.procfs=/host/proc"
      - "--path.rootfs=/rootfs"
      - "--path.sysfs=/host/sys"
      - "--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)"
    ports:
      - "9101:9100"
    networks:
      - atoma-network

  otel-collector:
    image: otel/opentelemetry-collector-contrib:0.119.0
    platform: ${PLATFORM:-}
    command: ["--config=/etc/otel-collector-config.yaml"]
    volumes:
      - ./otel-collector-config.yaml:/etc/otel-collector-config.yaml
    ports:
      - "4317:4317" # OTLP gRPC
      - "4318:4318" # OTLP HTTP
      - "8889:8889" # Prometheus exporter
    networks:
      - atoma-network
    depends_on:
      prometheus:
        condition: service_healthy
        required: true
      grafana:
        condition: service_healthy
        required: true
      loki:
        condition: service_healthy
        required: true
      tempo:
        condition: service_healthy
        required: true

  atoma-node:
    <<: *atoma-node
    profiles:
      - gpu
    ports:
      - "${ATOMA_SERVICE_PORT:-3000}:3000"
      - "127.0.0.1:${ATOMA_DAEMON_PORT:-3001}:3001"
      - "${ATOMA_P2P_PORT:-4001}:4001/udp"
      - "${ATOMA_P2P_PORT:-4001}:4001/tcp"
    depends_on:
      postgres-db:
        condition: service_started
        required: true
      vllm1:
        condition: service_started
        required: false
      vllm2:
        condition: service_started
        required: false
      vllm3:
        condition: service_started
        required: false
      vllm4:
        condition: service_started
        required: false
      vllm5:
        condition: service_started
        required: false
      vllm6:
        condition: service_started
        required: false
      vllm7:
        condition: service_started
        required: false
      vllm8:
        condition: service_started
        required: false
      vllm-cpu:
        condition: service_started
        required: false
      vllm-rocm:
        condition: service_started
        required: false
      mistralrs-cpu:
        condition: service_started
        required: false
      tei:
        condition: service_started
        required: false
      mistralrs:
        condition: service_started
        required: false
      otel-collector:
        condition: service_started
        required: true

  atoma-node-sglang:
    <<: *atoma-node
    profiles:
      - gpu
    ports:
      - "${ATOMA_SERVICE_SGLANG_PORT:-50000}:3000"
      - "127.0.0.1:${ATOMA_DAEMON_PORT:-3001}:3001"
      - "${ATOMA_P2P_PORT:-4001}:4001/udp"
      - "${ATOMA_P2P_PORT:-4001}:4001/tcp"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    depends_on:
      sglang:
        condition: service_started
        required: false
      postgres-db:
        condition: service_started
        required: true
      otel-collector:
        condition: service_started
        required: true

  atoma-node-no-nvidia:
    <<: *atoma-node-no-nvidia
    profiles:
      - no-gpu
    ports:
      - "${ATOMA_SERVICE_PORT:-3000}:3000"
      - "127.0.0.1:${ATOMA_DAEMON_PORT:-3001}:3001"
      - "${ATOMA_P2P_PORT:-4001}:4001/udp"
      - "${ATOMA_P2P_PORT:-4001}:4001/tcp"
    depends_on:
      postgres-db:
        condition: service_started
        required: true
      vllm:
        condition: service_started
        required: false
      vllm-cpu:
        condition: service_started
        required: false
      otel-collector:
        condition: service_started
        required: true

  sglang:
    container_name: sglang
    profiles: [chat_completions_sglang]
    image: lmsysorg/sglang:latest
    volumes:
      - ${HOME}/.cache/huggingface:/root/.cache/huggingface
    restart: always
    network_mode: host # required by RDMA
    privileged: true # required by RDMA
    environment:
      HF_TOKEN: ${HF_TOKEN}
    entrypoint: python3 -m sglang.launch_server
    command:
      --model-path ${SGLANG_MODEL_PATH} --tp 8 --enable-dp-attention --dp 8 --trust-remote-code --max-running-requests 128 --enable-metrics
      --host 0.0.0.0
      --port 3000
    ulimits:
      memlock: -1
      stack: 67108864
    ipc: host
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:3000/health || exit 1"]
    depends_on:
      sglang-deepgemm-precompile:
        condition: service_completed_successfully
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  sglang-deepgemm-precompile:
    container_name: sglang-deepgemm-precompiled
    profiles: [chat_completions_sglang]
    image: lmsysorg/sglang:latest
    volumes:
      - ${HOME}/.cache/huggingface:/root/.cache/huggingface
    environment:
      HF_TOKEN: ${HF_TOKEN}
    entrypoint: python3 -m sglang.compile_deep_gemm
    command:
      --model-path ${SGLANG_MODEL_PATH} --tp 8 --trust-remote-code
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  vllm1:
    <<: *inference-service-cuda
    container_name: chat-completions1
    profiles: [chat_completions_vllm]
    image: vllm/vllm-openai:v0.8.1
    environment:
      # Backend for attention computation
      # Available options:
      # - "TORCH_SDPA": use torch.nn.MultiheadAttention
      # - "FLASH_ATTN": use FlashAttention
      # - "XFORMERS": use XFormers
      # - "ROCM_FLASH": use ROCmFlashAttention
      # - "FLASHINFER": use flashinfer (recommended for fp8 quantized models)
      - VLLM_ATTENTION_BACKEND=FLASH_ATTN
      - VLLM_FLASH_ATTN_VERSION=3
      - VLLM_USE_V1=1
      - CUDA_VISIBLE_DEVICES=1
    ipc: host
    command: ${VLLM_ENGINE_ARGS}

  vllm2:
    <<: *inference-service-cuda
    container_name: chat-completions2
    profiles: [chat_completions_vllm]
    image: vllm/vllm-openai:v0.8.1
    environment:
      # Backend for attention computation
      # Available options:
      # - "TORCH_SDPA": use torch.nn.MultiheadAttention
      # - "FLASH_ATTN": use FlashAttention
      # - "XFORMERS": use XFormers
      # - "ROCM_FLASH": use ROCmFlashAttention
      # - "FLASHINFER": use flashinfer (recommended for fp8 quantized models)
      - VLLM_ATTENTION_BACKEND=FLASH_ATTN
      - VLLM_FLASH_ATTN_VERSION=3
      - VLLM_USE_V1=1
      - CUDA_VISIBLE_DEVICES=2
    ipc: host
    command: ${VLLM_ENGINE_ARGS}

  vllm3:
    <<: *inference-service-cuda
    container_name: chat-completions3
    profiles: [chat_completions_vllm]
    image: vllm/vllm-openai:v0.8.1
    environment:
      # Backend for attention computation
      # Available options:
      # - "TORCH_SDPA": use torch.nn.MultiheadAttention
      # - "FLASH_ATTN": use FlashAttention
      # - "XFORMERS": use XFormers
      # - "ROCM_FLASH": use ROCmFlashAttention
      # - "FLASHINFER": use flashinfer (recommended for fp8 quantized models)
      - VLLM_ATTENTION_BACKEND=FLASH_ATTN
      - VLLM_FLASH_ATTN_VERSION=3
      - VLLM_USE_V1=1
      - CUDA_VISIBLE_DEVICES=3
    ipc: host
    command: ${VLLM_ENGINE_ARGS}

  vllm4:
    <<: *inference-service-cuda
    container_name: chat-completions4
    profiles: [chat_completions_vllm]
    image: vllm/vllm-openai:v0.8.1
    environment:
      # Backend for attention computation
      # Available options:
      # - "TORCH_SDPA": use torch.nn.MultiheadAttention
      # - "FLASH_ATTN": use FlashAttention
      # - "XFORMERS": use XFormers
      # - "ROCM_FLASH": use ROCmFlashAttention
      # - "FLASHINFER": use flashinfer (recommended for fp8 quantized models)
      - VLLM_ATTENTION_BACKEND=FLASH_ATTN
      - VLLM_FLASH_ATTN_VERSION=3
      - VLLM_USE_V1=1
      - CUDA_VISIBLE_DEVICES=4
    ipc: host
    command: ${VLLM_ENGINE_ARGS}

  vllm5:
    <<: *inference-service-cuda
    container_name: chat-completions5
    profiles: [chat_completions_vllm]
    image: vllm/vllm-openai:v0.8.1
    environment:
      # Backend for attention computation
      # Available options:
      # - "TORCH_SDPA": use torch.nn.MultiheadAttention
      # - "FLASH_ATTN": use FlashAttention
      # - "XFORMERS": use XFormers
      # - "ROCM_FLASH": use ROCmFlashAttention
      # - "FLASHINFER": use flashinfer (recommended for fp8 quantized models)
      - VLLM_ATTENTION_BACKEND=FLASH_ATTN
      - VLLM_FLASH_ATTN_VERSION=3
      - VLLM_USE_V1=1
      - CUDA_VISIBLE_DEVICES=5
    ipc: host
    command: ${VLLM_ENGINE_ARGS}

  vllm6:
    <<: *inference-service-cuda
    container_name: chat-completions6
    profiles: [chat_completions_vllm]
    image: vllm/vllm-openai:v0.8.1
    environment:
      # Backend for attention computation
      # Available options:
      # - "TORCH_SDPA": use torch.nn.MultiheadAttention
      # - "FLASH_ATTN": use FlashAttention
      # - "XFORMERS": use XFormers
      # - "ROCM_FLASH": use ROCmFlashAttention
      # - "FLASHINFER": use flashinfer (recommended for fp8 quantized models)
      - VLLM_ATTENTION_BACKEND=FLASH_ATTN
      - VLLM_FLASH_ATTN_VERSION=3
      - VLLM_USE_V1=1
      - CUDA_VISIBLE_DEVICES=6
    ipc: host
    command: ${VLLM_ENGINE_ARGS}

  vllm7:
    <<: *inference-service-cuda
    container_name: chat-completions7
    profiles: [chat_completions_vllm]
    image: vllm/vllm-openai:v0.8.1
    environment:
      # Backend for attention computation
      # Available options:
      # - "TORCH_SDPA": use torch.nn.MultiheadAttention
      # - "FLASH_ATTN": use FlashAttention
      # - "XFORMERS": use XFormers
      # - "ROCM_FLASH": use ROCmFlashAttention
      # - "FLASHINFER": use flashinfer (recommended for fp8 quantized models)
      - VLLM_ATTENTION_BACKEND=FLASH_ATTN
      - VLLM_FLASH_ATTN_VERSION=3
      - VLLM_USE_V1=1
      - CUDA_VISIBLE_DEVICES=7
    ipc: host
    command: ${VLLM_ENGINE_ARGS}

  vllm8:
    <<: *inference-service-cuda
    container_name: chat-completions8
    profiles: [chat_completions_vllm]
    image: vllm/vllm-openai:v0.8.1
    environment:
      # Backend for attention computation
      # Available options:
      # - "TORCH_SDPA": use torch.nn.MultiheadAttention
      # - "FLASH_ATTN": use FlashAttention
      # - "XFORMERS": use XFormers
      # - "ROCM_FLASH": use ROCmFlashAttention
      # - "FLASHINFER": use flashinfer (recommended for fp8 quantized models)
      - VLLM_ATTENTION_BACKEND=FLASH_ATTN
      - VLLM_FLASH_ATTN_VERSION=3
      - VLLM_USE_V1=1
      - CUDA_VISIBLE_DEVICES=8
    ipc: host
    command: ${VLLM_ENGINE_ARGS}

  vllm-cpu:
    <<: *inference-service-cpu
    container_name: chat-completions
    profiles: [chat_completions_vllm_cpu]
    build:
      context: https://github.com/atoma-network/vllm.git#main
      dockerfile: Dockerfile.cpu
    command: --model ${CHAT_COMPLETIONS_MODEL} --max-model-len ${CHAT_COMPLETIONS_MAX_MODEL_LEN}

  vllm-rocm:
    <<: *inference-service-rocm
    container_name: chat-completions
    profiles: [chat_completions_vllm_rocm]
    build:
      context: https://github.com/atoma-network/vllm.git#main
      dockerfile: Dockerfile.rocm
    command: --model ${CHAT_COMPLETIONS_MODEL} --max-model-len ${CHAT_COMPLETIONS_MAX_MODEL_LEN} --tensor-parallel-size ${VLLM_TENSOR_PARALLEL_SIZE}

  mistralrs-cpu:
    <<: *inference-service-cpu
    container_name: chat-completions
    profiles: [chat_completions_mistralrs_cpu]
    image: ghcr.io/ericlbuehler/mistral.rs:cpu-0.4
    platform: linux/amd64
    command: plain -m ${CHAT_COMPLETIONS_MODEL}

  tei:
    <<: *inference-service-cuda
    container_name: embeddings
    profiles: [embeddings_tei]
    image: ${TEI_IMAGE}
    command: --model-id ${EMBEDDINGS_MODEL} --huggingface-hub-cache /root/.cache/huggingface/hub

  mistralrs:
    <<: *inference-service-cuda
    container_name: image-generations
    profiles: [image_generations_mistralrs]
    image: ${MISTRALRS_IMAGE}
    platform: ${PLATFORM:-}
    command: diffusion-plain -m ${IMAGE_GENERATIONS_MODEL} --arch ${IMAGE_GENERATIONS_ARCHITECTURE}

networks:
  atoma-network:
    driver: bridge
    name: atoma-network

volumes:
  postgres-data:
  grafana_data:
  sui-config-volume:
  prometheus-data:
  loki-data:
  tempo-data:
  atoma-local-key:
